* resources
  - https://en.wikipedia.org/wiki/List_of_convolutions_of_probability_distributions
  - https://en.wikipedia.org/wiki/Irwin–Hall_distribution 

* goal
  
  #+begin_quote
  rewrite Prob PL program expression to distribution queries
  #+end_quote

* examples
** identities 
   
   - irwin hall 
     
     #+begin_example
     sum = 0;
     for i in 1..2 do
       sum = sum + rand(0, 1)
     #+end_example

     #+begin_example
     sum = 0;
     sum = sum + rand(0, 1);
     sum = sum + rand(0, 1)
     #+end_example

     #+begin_example
     assignment 1:

     --------------
     sum = 0 : nat


     assignment 2:

     sum : nat    rand(0, 1) : dist uniform 0 1    iid(sum, rand(0, 1))
     -----------------------------------------------------------------
     sum + rand(0, 1) : dist uniform sum sum+1


     assignment 3:

     sum : dist uniform sum sum+1     rand(0, 1) : dist uniform 0 1    iid(sum, rand(0, 1))
     --------------------------------------------------------------------------------------
     sum + rand(0, 1) : dist iw ...
     #+end_example

** CTL 
   
   - given a "large enough" set of samples from distribution with the same
     expectation and stdev we can replace that with the normal distribution
     around the expectation 

     #+begin_example
     for i 1..n do
       sum = sum + dist(u, sig) * i 
     #+end_example
     
     we can factor out dist to ~norm(u, sig)~
     
     #+begin_example
     for i 1..n do
       sum = sum + i

     sum * norm(u, sig)
     #+end_example
   
* framework

  #+begin_example
  canon e D <=> e is the canonical expression representing a distribution

  eg,
    canon rand(0,1) (dist uniform 0 1)
    canon (rand(0,1) * (b-a) + a) (dist uniform a b)

  #V(v) = count of v in V

  pdf_e(v) = #V(v)/|V|
    where V = { v' | e || v' } (multiset)
          |V| -> inf

  pdf_e = pdf_e' <=> forall v, pdf_e(v) = pdf_e'(v)
  pdf_e(v) = D(v) <=> canon ec D /\ pdf_ec(v) = pdf_e(v)

  Transformation Soundness

  if [[e]] = e' then pdf_e = pdf_e'

  (1) Type soundness

  if e : D and e || v then pdf_e(v) = D(v)

  (2) Transformation type preservation

  if [[e]] = e' and [[e]] : D then e' : D
  #+end_example
* task
** questions
*** DONE why are random variables measurable functions?
    CLOSED: [2017-11-22 Wed 20:50]
    #+begin_example
    # probability space
    W1 = {HH, HT, TH, TT}
    Sig1 = Pow(W1) = {{HH}, {HT}, {TH}, {TT}, {HH, HT}, {HH, TH}, ... }

    # state space
    W2 = {0, 1, 2}
    Sig2 = Pow(W2) = {{0}, {1}, {2}, {0,1} ... {0, 1, 2}}

    # is the preimage constraint satisfied? Examples
    # E in Sig2 
    # {0, 1} inverse of X on each element {TT, HT} is in Sig1
    #                                also {TT, TH} is in Sig1
    # 
    # It's not really the inverse of the function, from the wikipedia page
    #
    # f^-1(E) := {x in X | f(x) in E } in Sig1 forall E in Sig2
    #
    # It operates on subsets of the second measurable space
    
    X : W1 -> W2
    P : Sig1 -> [0,1]

    X(HH) = 2 
    X(HT) = 1
    X(TH) = 1
    X(TT) = 0

    P({HH}) = 0.25
    P({HT}) = 0.25
    P({TH}) = 0.25
    P({TT}) = 0.25
    P({HH, HT}) = 0.50
    ...
    P({HH, HT, TH}) = 0.75 
    ...
    P({HH, HT, TH, TT}) = 1

    In which case 

    # intuition
    Pr(X > 0) = Pr({HH, HT, TH}) = 0.75

    # from wikipedia
    # If A ⊂ S, the notation Pr(X ∈ A) is a commonly used shorthand for P({ω ∈ Ω: X(ω) ∈ A}).
    # Pr({w in W : X(w) in {1, 2}
    Pr(X > 0)
      = Pr(X in {1, 2})
      = Pr({w in W : X(w) in {1, 2})
      = Pr({HH, HT, TH})
      = 0.75
    #+end_example

*** DONE what is the intuition behind convolution of discrete random variables?
    CLOSED: [2017-11-24 Fri 12:54]

    The cartesian product of the two state spaces and the corresponding product
    of probabilities. Convolution gets the sum of the product of probabilites 
    at any given point for the new variable. 

    Sum of the probabilities of all the ways to make the new variables take its
    value as a function of the other two variables.

    At every discreat point sum the probabilities of all the way to make the
    additive outcome of the variables equal that point.

    Borrowing the random variables example from above:

    #+begin_example
    # probability space
    W1 = {HH, HT, TH, TT}
    Sig1 = Pow(W1) = {{HH}, {HT}, {TH}, {TT}, {HH, HT}, {HH, TH}, ... }

    # state space
    W2 = {0, 1, 2}
    Sig2 = Pow(W2) = {{0}, {1}, {2}, {0,1} ... {0, 1, 2}}

    # two random variables 
    X : W1 -> W2
    Y : W1 -> W2
    P : Sig1 -> [0,1]

    # first coint flipped twice, number heads
    X(HH) = 2 
    X(HT) = 1
    X(TH) = 1
    X(TT) = 0

    # second coin flipped twice, number tails
    Y(HH) = 0 
    Y(HT) = 1
    Y(TH) = 1
    Y(TT) = 2

    P({HH}) = 0.25
    P({HT}) = 0.25
    P({TH}) = 0.25
    P({TT}) = 0.25
    P({HH, HT}) = 0.50
    ...
    P({HH, HT, TH}) = 0.75 
    ...
    P({HH, HT, TH, TT}) = 1

    # defnition of Z = X + Y, both coins flipped twice?
    Z(w, w) = X(w) + Y(w)

    ww = W1 x W2 = {(HH, HH), (HH, HT), (HH, TH), (HH, TT) ... (TT, TT)}
    Sig3 = Pow(ww)

    # P by construction
    P({HH}, {HH}) = P({HH}) * P({HH})
    P({TT}, {TT}) = P({TT}) * P({TT})
    ...

    # also we can define the probability distribution through convolution
    P(Z = z) = sum(k, -inf, inf, Pr(X = k)*Pr(Y = z-k))

    # probability that both counts sum to 1 
    P(Z = 4) =
      ... + # Pr of all worlds where X < 0, zero worlds, zero probability 
      Pr(X = 0)*Pr(Y = 4) + # There are no worlds where Y = 4 so zero prob
      Pr(X = 1)*Pr(Y = 3) + # there are no worlds where Y = 3 so zero prob
      Pr(X = 2)*Pr(Y = 2) + # = 0.25 * 0.25 = Pr({HH}) * Pr({TT})
      Pr(X = 3)*Pr(Y = 1) + # there are no worlds where X = 3 so zero prob
      Pr(X = 4)*Pr(Y = 0) + # there are no worlds where X = 4 so zero prob
      ... # Pr of all worlds where X > 2, zero worlds, zero probability 

    # probability that the counts sum to 2
    Pr(Z = 2) =
      ... + # Pr of all worlds where X < 0, zero worlds, zero probability 
      Pr(X = 0)*Pr(Y = 2) + # = 0.25 * 0.25 = Pr({TT}) * Pr({TT}) = Pr({(TT, TT)})
      Pr(X = 1)*Pr(Y = 1) + # = 0.75 * 0.75 = Pr(w1 = {HT, TH}) * Pr(w2 = {HT, TH}) = Pr(w1 x w2)
      Pr(X = 2)*Pr(Y = 0) + # = 0.25 * 0.25 = Pr({HH}) * Pr({HH})
      ... # Pr of all worlds where X > 2, zero worlds, zero probability
    #+end_example

*** TODO why are functions from A -> U in coq measurable?
    - ~U = [0, 1]~
    - why do they admit an inverse?
      - not all coq functions do since for example, one can encode addition in coq
      - it's not clear that they need to, see question about random variables as
        measurable functions
    - why do they always satisfy the preimage constraint ~(W, Sig), for x in W, f^-1(x) in Sig~?
    - why is the preimage constraint important?
      - idea: prevents "the measure problem" as described by Tao in his book.
        this would prevent the "doubling" of the measure using the function f(x)
        = 2x for f : A -> B where A = [0,1] and B = [0,1] since every point in
        the target metric space [0,2] does not appear in the source space [0,1]
*** TODO why are sequenced statements convolved in Steven's semantics?
    - ie, why does sequencing sum distributions?
    - doesn't the substitution of the variable mean they are not independent?
      
      It seems to follow the wikipedia definition of convolution for two random
      variables but that requires independence. Doesn't the substitution of the 
      random variable make the second expression dependent on the first?
      
    - what is the intuition behind convolution as a sum of distributions?
    - are let statements convolved in the ALEA semantics?
*** DONE what does the predicate `ok` mean?
    CLOSED: [2017-11-27 Mon 11:33]
    - given a lower bound ~p~, the sum of the probabilities in the set
      represented by alpha filtered by ~q~ for distribution ~e~ is greater.
*** DONE why does a proof of `ok` for the given values imply termination for iterflip?
    CLOSED: [2017-11-27 Mon 11:33]
    - Based on our understanding of the distribution, if the sum of
      probabilities for all values in Z is 1 then all of them have some
      probability of being the return value. But because the function is pure,
      non-termination would imply that some value has the probability zero.
*** DONE why is this relevant to optimizing transformations?
    CLOSED: [2017-11-27 Mon 11:33]
    - we need to prove equivalence in the limit of expressions so we need 
      machinary like that in the proof of termination for iterflip
** formalize language 
** logic for data flow
*** TODO pure language extensions (if else, addition)
*** TODO define rules
** library for reasoning about distributions
*** 
* progress
** okfun (is it really though?)
*** iterflip almost surely terminates
    #+begin_example
    (* iterflip : Z -> distr Z *)
    letrec iterflip x = if flip() then iterflip (x+1) else x
    #+end_example

*** almost surely terminates
    
    #+begin_example
    Lemma iterflip_term : okfun (fun k => 1) iterflip q1.

    Definition okfun (A B:Type)(p:A -> U)(e:A -> distr B)(q:A -> B -> U)
      := forall x:A, ok (p x) (e x) (q x).

    Definition ok (A:Type) (p:U) (e:distr A) (q:A -> U) 
      := p <= mu e q.

    (** mu : (distr Z) -> ((Z -m> [0,1]) -m> [0,1]) 
        q = fun z => 1.
        p = 1 
        mu e : (Z -m> [0,1]) -m> [0,1]  
        mu e q : [0,1] *)
    #+end_example

*** core ideas
    - ~p~ defines a lower bound
    - ~q~ defines a subset of the sample space ~Z~ (as a "filter")
    - ~mu~ is a measure over subsets of ~Z~, it defines a distribution over ~Z~
    - ~p <= mu e q~, the probability sum for all elements of ~Z~ is 1

** a measure of measure theory
   - measurable space, ~(W, Sig)~
     - sample space ~W~
       - eg, two flips of the same coin Wc = {HH, HT, TH, TT}
     - sigma algebra ~Sig~
       - set of sets with elements drawn from sample space
       - eg, the powerset of Wc 
       - closed under countable union/intersection and complementation
       - includes empty set
       - why these constraints? 
         - uniformity of the set avoids problems with "measuring"
   - measurable function, ~f: W1 -> W2~
     - maping between sample spaces
     - can be thought of as a filter on W1
     - if ~S in Sig2~ then ~{ x | f(x) in S }~ must be in ~Sig1~
       - sometimes ~f: (W1, Sig1) -> (W2, Sig2)~
     - eg, random variable X "# of heads"
   - probability measure
     - measure from sigma algebra to unit interval [0,1]
     - ~mu(empty) = 0~
     - ~mu(A) = n, n >= 0~ for all A in Sig
     - additive union, ~mu(U Si) = sum mu(si)~
     - ~mu(W) = 1~
   - probability space
     - measurable space equiped with a probability measure
   - Example: random variable as a measurable function, two coin tosses
     - ~(W1, Sig1)~
       #+begin_example
       W1 = {HH, HT, TH, TT}
       Sig1 = Pow(W1) = {{HH}, {HT}, {TH}, {TT}, {HH, HT}, {HH, TH}, ... }
       #+end_example
     - ~(W2, Sig2)~
       #+begin_example
       W2 = {0, 1, 2}
       Sig2 = Pow(W2) = {{0}, {1}, {2}, {0,1} ... {0, 1, 2}}
       #+end_example
     - Random variable ~X~ 
       #+begin_example
       X : W1 -> W2

       X(HH) = 2 
       X(HT) = 1
       X(TH) = 1
       X(TT) = 0
       #+end_example
     - Probability measure ~P~ 
       #+begin_example
       P : Sig1 -> [0,1]

       P({HH}) = 0.25
       P({HT}) = 0.25
       P({TH}) = 0.25
       P({TT}) = 0.25
       P({HH, HT}) = 0.50
       ...
       P({HH, HT, TH}) = 0.75 
       ...
       P({HH, HT, TH, TT}) = 1
       #+end_example
     - Queries on the random variable
       #+begin_example
       # intuition
       Pr(X > 0) = Pr({HH, HT, TH}) = 0.75

       # Using X as a map to a measurable space
       # 1. define the set of elements in the codomain that satisfy the query {1,2}
       # 2. generate the set of element in the domain that result in elements in the codomain
       # 3. apply the probability measure to that set
       # Pr({w in W : X(w) in {1, 2}})
       Pr(X > 0)
         = Pr(X in {1, 2})
         = Pr({w in W : X(w) in {1, 2})
         = Pr({HH, HT, TH})
         = 0.75
       #+end_example
** probability monad
*** haskel
    #+begin_example
    newtype Probability = P Float
    newtype Dist a = D {unD :: [(a,Probability)]}

    type Event a = a -> Bool
    (??) :: Event a -> Dist a -> Probability
    (??) p = P . sum . map snd . filter (p . fst) . unD
    
    class Monad m where
      (>>=)  :: m a -> (a -> m b) -> m b
      (>>)   :: m a -> m b -> m b
      return :: a -> m a
      fail   :: String -> m a  

    instance Monad Dist where
      return x    = D [(x,1)]
      (D d) >>= f = D [(y,q*p) | (x,p) <- d, (y,q) <- unD (f x)]
      fail        = D []
    #+end_example

*** alea 
    #+begin_example
    Definition MF (A:Type) := A -m> [0,1].
    Definition M (A:Type) := MF A -m> [0,1].
    (** Definition M (A:Type) := (A -m> [0,1]) -m> [0,1]. *)

    Definition unit (A:Type) (x:A) : M A.
    Definition star : forall (A B:Type), M A -> (A -> M B) -> M B.
    #+end_example

** review
*** almost surely terminates
    
    #+begin_example
    Lemma iterflip_term : okfun (fun k => 1) iterflip q1.

    Definition okfun (A B:Type)(p:A -> U)(e:A -> distr B)(q:A -> B -> U)
      := forall x:A, ok (p x) (e x) (q x).

    Definition ok (A:Type) (p:U) (e:distr A) (q:A -> U) 
      := p <= mu e q.

    (** mu : (distr Z) -> ((Z -m> [0,1]) -m> [0,1]) 
        q = fun z => 1.
        p = 1 
        mu e : (Z -m> [0,1]) -m> [0,1]  
        mu e q : [0,1] *)
    #+end_example

*** core ideas
    - ~p~ defines a lower bound
    - ~q~ defines a subset of the sample space ~Z~ (as a "filter")
    - ~mu~ is a measure over subsets of ~Z~, it defines a distribution over ~Z~
    - ~p <= mu e q~, the probability sum for all elements of ~Z~ is 1

    - if we show that the sum of the measure defined by ~iterflip~ for all
      elements in ~Z~ is 1 and we know that every element has a nonzero
      probability then it must be that every element has a chance of being the
      return value. Otherwise some element in the set would be excluded by
      always terminating early (don't care) or by being skipped in a
      non-terminating execution (do care).
